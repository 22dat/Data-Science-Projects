{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-iuRLRs_27ki"
   },
   "source": [
    "# **Springboard Decision Tree Specialty Coffee Case Study - Tier 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JqX4KRR327kl"
   },
   "source": [
    "# The Scenario\n",
    "\n",
    "Imagine you've just finished the Springboard Data Science Career Track course, and have been hired by a rising popular specialty coffee company - RR Diner Coffee - as a data scientist. Congratulations!\n",
    "\n",
    "RR Diner Coffee sells two types of thing:\n",
    "- specialty coffee beans, in bulk (by the kilogram only) \n",
    "- coffee equipment and merchandise (grinders, brewing equipment, mugs, books, t-shirts).\n",
    "\n",
    "RR Diner Coffee has three stores, two in Europe and one in the USA. The flagshap store is in the USA, and everything is quality assessed there, before being shipped out. Customers further away from the USA flagship store have higher shipping charges. \n",
    "\n",
    "You've been taken on at RR Diner Coffee because the company are turning towards using data science and machine learning to systematically make decisions about which coffee farmers they should strike deals with. \n",
    "\n",
    "RR Diner Coffee typically buys coffee from farmers, processes it on site, brings it back to the USA, roasts it, packages it, markets it, and ships it (only in bulk, and after quality assurance) to customers internationally. These customers all own coffee shops in major cities like New York, Paris, London, Hong Kong, Tokyo, and Berlin. \n",
    "\n",
    "Now, RR Diner Coffee has a decision about whether to strike a deal with a legendary coffee farm (known as the **Hidden Farm**) in rural China: there are rumours their coffee tastes of lychee and dark chocolate, while also being as sweet as apple juice. \n",
    "\n",
    "It's a risky decision, as the deal will be expensive, and the coffee might not be bought by customers. The stakes are high: times are tough, stocks are low, farmers are reverting to old deals with the larger enterprises and the publicity of selling *Hidden Farm* coffee could save the RR Diner Coffee business. \n",
    "\n",
    "Your first job, then, is ***to build a decision tree to predict how many units of the Hidden Farm Chinese coffee will be purchased by RR Diner Coffee's most loyal customers.*** \n",
    "\n",
    "To this end, you and your team have conducted a survey of 710 of the most loyal RR Diner Coffee customers, collecting data on the customers':\n",
    "- age\n",
    "- gender \n",
    "- salary \n",
    "- whether they have bought at least one RR Diner Coffee product online\n",
    "- their distance from the flagship store in the USA (standardized to a number between 0 and 11) \n",
    "- how much they spent on RR Diner Coffee products on the week of the survey \n",
    "- how much they spent on RR Diner Coffee products in the month preeding the survey\n",
    "- the number of RR Diner coffee bean shipments each customer has ordered over the preceding year. \n",
    "\n",
    "You also asked each customer participating in the survey whether they would buy the Hidden Farm coffee, and some (but not all) of the customers gave responses to that question. \n",
    "\n",
    "You sit back and think: if more than 70% of the interviewed customers are likely to buy the Hidden Farm coffee, you will strike the deal with the local Hidden Farm farmers and sell the coffee. Otherwise, you won't strike the deal and the Hidden Farm coffee will remain in legends only. There's some doubt in your mind about whether 70% is a reasonable threshold, but it'll do for the moment. \n",
    "\n",
    "To solve the problem, then, you will build a decision tree to implement a classification solution. \n",
    "\n",
    "\n",
    "-------------------------------\n",
    "As ever, this notebook is **tiered**, meaning you can elect that tier that is right for your confidence and skill level. There are 3 tiers, with tier 1 being the easiest and tier 3 being the hardest. This is ***tier 1***, so a gentle introduction to classification models using decision trees. \n",
    "\n",
    "**1. Sourcing and loading** \n",
    "- Import packages\n",
    "- Load data\n",
    "- Explore the data\n",
    "\n",
    " \n",
    "**2. Cleaning, transforming and visualizing**\n",
    "- Cleaning the data\n",
    "- Train/test split\n",
    "  \n",
    "  \n",
    "**3. Modelling** \n",
    "- Model 1: Entropy model - no max_depth\n",
    "- Model 2: Gini impurity model - no max_depth\n",
    "- Model 3: Entropy model - max depth 3\n",
    "- Model 4: Gini impurity model - max depth 3\n",
    "\n",
    "\n",
    "**4. Evaluating and concluding** \n",
    "- How many customers will buy Hidden Farm coffee?\n",
    "- Decision\n",
    "\n",
    "**5. Random Forest** \n",
    "- Import necessary modules\n",
    "- Model\n",
    "- Revise conclusion\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zTlwi4M27km"
   },
   "source": [
    "# 0. Overview\n",
    "\n",
    "This notebook uses decision trees to determine whether the factors of salary, gender, age, how much money the customer spent last week and during the preceding month on RR Diner Coffee products, how many kilogram coffee bags the customer bought over the last year, whether they have bought at least one RR Diner Coffee product online, and their distance from the flagship store in the USA, could predict whether customers would purchase the Hidden Farm coffee if a deal with its farmers were struck. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GuKgPSPS27ko"
   },
   "source": [
    "# 1. Sourcing and loading\n",
    "## 1a. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GsP21kkU27kq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import _ _.pyplot as plt\n",
    "from io import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLV5cfvy27kx"
   },
   "source": [
    "## 1b. Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iaAFg7J127kx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicho\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\nicho\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\nicho\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# Read in the data to a variable called coffeeData\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the data from the CSV file\n",
    "coffeeData = pd.read_csv(\"RRDinerCoffeeData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJJHzGb127k1"
   },
   "source": [
    "## 1c. Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8B-HKvF627k1"
   },
   "source": [
    "As we've seen, exploration entails doing things like checking out the **initial appearance** of the data with head(), the **dimensions** of our data with .shape, the **data types** of the variables with .info(), the **number of non-null values**, how much **memory** is being used to store the data, and finally the major summary statistcs capturing **central tendancy, dispersion and the null-excluding shape of the dataset's distribution**. \n",
    "\n",
    "How much of this can you do yourself by this point in the course? Have a real go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5JmlDkE027k2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XBGDWgCR27k6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>num_coffeeBags_per_year</th>\n",
       "      <th>spent_week</th>\n",
       "      <th>spent_month</th>\n",
       "      <th>SlrAY</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Online</th>\n",
       "      <th>Decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>73</td>\n",
       "      <td>42789</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>164</td>\n",
       "      <td>74035</td>\n",
       "      <td>0.520906</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>119</td>\n",
       "      <td>30563</td>\n",
       "      <td>0.916005</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>107</td>\n",
       "      <td>13166</td>\n",
       "      <td>0.932098</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>36</td>\n",
       "      <td>14244</td>\n",
       "      <td>0.965881</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>14293</td>\n",
       "      <td>1.036346</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>202</td>\n",
       "      <td>91035</td>\n",
       "      <td>1.134851</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>17425</td>\n",
       "      <td>1.193188</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Gender  num_coffeeBags_per_year  spent_week  spent_month  SlrAY  \\\n",
       "0   36  Female                        0          24           73  42789   \n",
       "1   24    Male                        0          44          164  74035   \n",
       "2   24    Male                        0          39          119  30563   \n",
       "3   20    Male                        0          30          107  13166   \n",
       "4   24  Female                        0          20           36  14244   \n",
       "5   20  female                        0          23           28  14293   \n",
       "6   34  Female                        0          55          202  91035   \n",
       "7   24  Female                        0          20           34  17425   \n",
       "\n",
       "   Distance  Online  Decision  \n",
       "0  0.003168       0       1.0  \n",
       "1  0.520906       0       NaN  \n",
       "2  0.916005       1       1.0  \n",
       "3  0.932098       1       NaN  \n",
       "4  0.965881       0       1.0  \n",
       "5  1.036346       1       1.0  \n",
       "6  1.134851       0       1.0  \n",
       "7  1.193188       0       NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffeeData.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZH6lJWU27k8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 702 entries, 0 to 701\n",
      "Data columns (total 9 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Age                      702 non-null    int64  \n",
      " 1   Gender                   702 non-null    object \n",
      " 2   num_coffeeBags_per_year  702 non-null    int64  \n",
      " 3   spent_week               702 non-null    int64  \n",
      " 4   spent_month              702 non-null    int64  \n",
      " 5   SlrAY                    702 non-null    int64  \n",
      " 6   Distance                 702 non-null    float64\n",
      " 7   Online                   702 non-null    int64  \n",
      " 8   Decision                 474 non-null    float64\n",
      "dtypes: float64(2), int64(6), object(1)\n",
      "memory usage: 49.5+ KB\n"
     ]
    }
   ],
   "source": [
    "coffeeData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jqSVLUm327k_"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>num_coffeeBags_per_year</th>\n",
       "      <th>spent_week</th>\n",
       "      <th>spent_month</th>\n",
       "      <th>SlrAY</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Online</th>\n",
       "      <th>Decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>702.000000</td>\n",
       "      <td>702</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>474.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>34.243590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.710826</td>\n",
       "      <td>32.853276</td>\n",
       "      <td>107.923077</td>\n",
       "      <td>43819.843305</td>\n",
       "      <td>4.559186</td>\n",
       "      <td>0.531339</td>\n",
       "      <td>0.639241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.927945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.593629</td>\n",
       "      <td>15.731878</td>\n",
       "      <td>55.348485</td>\n",
       "      <td>26192.626943</td>\n",
       "      <td>3.116275</td>\n",
       "      <td>0.499373</td>\n",
       "      <td>0.480728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1617.000000</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.250000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>22812.250000</td>\n",
       "      <td>1.877812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>41975.000000</td>\n",
       "      <td>4.196167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>150.750000</td>\n",
       "      <td>60223.000000</td>\n",
       "      <td>6.712022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>182058.000000</td>\n",
       "      <td>10.986203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Age Gender  num_coffeeBags_per_year  spent_week  spent_month  \\\n",
       "count   702.000000    702               702.000000  702.000000   702.000000   \n",
       "unique         NaN      9                      NaN         NaN          NaN   \n",
       "top            NaN   Male                      NaN         NaN          NaN   \n",
       "freq           NaN    355                      NaN         NaN          NaN   \n",
       "mean     34.243590    NaN                 2.710826   32.853276   107.923077   \n",
       "std      13.927945    NaN                 1.593629   15.731878    55.348485   \n",
       "min      16.000000    NaN                 0.000000    0.000000     0.000000   \n",
       "25%      23.000000    NaN                 1.000000   24.250000    62.000000   \n",
       "50%      28.000000    NaN                 3.000000   36.000000   113.500000   \n",
       "75%      46.000000    NaN                 4.000000   43.000000   150.750000   \n",
       "max      90.000000    NaN                 5.000000   62.000000   210.000000   \n",
       "\n",
       "                SlrAY    Distance      Online    Decision  \n",
       "count      702.000000  702.000000  702.000000  474.000000  \n",
       "unique            NaN         NaN         NaN         NaN  \n",
       "top               NaN         NaN         NaN         NaN  \n",
       "freq              NaN         NaN         NaN         NaN  \n",
       "mean     43819.843305    4.559186    0.531339    0.639241  \n",
       "std      26192.626943    3.116275    0.499373    0.480728  \n",
       "min       1617.000000    0.003168    0.000000    0.000000  \n",
       "25%      22812.250000    1.877812    0.000000    0.000000  \n",
       "50%      41975.000000    4.196167    1.000000    1.000000  \n",
       "75%      60223.000000    6.712022    1.000000    1.000000  \n",
       "max     182058.000000   10.986203    1.000000    1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call describe() on your data with the parameter include = 'all' to get the relevant summary statistics for your data \n",
    "coffeeData.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IKleHghD27lC"
   },
   "source": [
    "# 2. Cleaning, transforming and visualizing\n",
    "## 2a. Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rrXQN3jK27lF"
   },
   "source": [
    "Some datasets don't require any cleaning, but almost all do. This one does. We need to replace '1.0' and '0.0' in the 'Decision' column by 'YES' and 'NO' respectively, clean up the values of the 'gender' column, and change the column names to words which maximize meaning and clarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZVP2hxf27lG"
   },
   "source": [
    "First, let's change the name of `spent_week`, `spent_month`, and `SlrAY` to `spent_last_week` and `spent_last_month` and `salary` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iiR6dxn427lH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Gender', 'num_coffeeBags_per_year', 'spent_week', 'spent_month',\n",
       "       'SlrAY', 'Distance', 'Online', 'Decision'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call .columns on your data to check out the names of our data's columns \n",
    "coffeeData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AcGZ5jU527lK"
   },
   "outputs": [],
   "source": [
    "# Using .rename(), make the relevant name changes to spent_week and spent_per_week.\n",
    "# Remember: you can either do a reassignment, or use inplace=True. Both will change the value of coffeeData\n",
    "#coffeeData._ _ _(columns = {\"spent_month\":\"spent_last_month\", \"spent_week\":\"spent_last_week\", \"SlrAY\":\"Salary\"},\n",
    "           # inplace = True)\n",
    "\n",
    "coffeeData.rename(columns={\"spent_week\": \"spent_last_week\", \"spent_per_week\": \"spent_last_week\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4CHOpkA27lM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Gender', 'num_coffeeBags_per_year', 'spent_last_week',\n",
       "       'spent_month', 'SlrAY', 'Distance', 'Online', 'Decision'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the column names\n",
    "coffeeData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6XuI18FV27lQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      702\n",
       "unique       9\n",
       "top       Male\n",
       "freq       355\n",
       "Name: Gender, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a closer look at the gender column. Its values need cleaning.\n",
    "# Call describe() on the gender column \n",
    "coffeeData[\"Gender\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZPe4W_pM27lU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Female', 'Male', 'female', 'F', 'f ', 'FEMALE', 'MALE', 'male',\n",
       "       'M'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call unique() on the gender column to see its unique values \n",
    "coffeeData[\"Gender\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwEMG7MB27lY"
   },
   "source": [
    "We can see a bunch of inconsistency here.\n",
    "\n",
    "Use replace() to make the values of the `gender` column just `Female` and `Male`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "II50etW127lZ"
   },
   "outputs": [],
   "source": [
    "# Use the function .replace() on the column \"gender\"; replace all alternate values with 'Female'\n",
    "coffeeData[\"Gender\"] = coffeeData[\"Gender\"].replace([\"female\", \"f \", \"FEMALE\", \"F\"], \"Female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5N3wcWjO27lb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Female', 'Male', 'MALE', 'male', 'M'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the unique values of the column \"gender\"\n",
    "coffeeData[\"Gender\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nlpMhrAR27le"
   },
   "outputs": [],
   "source": [
    "# Use the function .replace() on the column \"gender\"; replace all alternate values with \"Male\"\n",
    "coffeeData[\"Gender\"] = coffeeData[\"Gender\"].replace([\"MALE\", \"male\", \"M\"], \"Male\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAoMxGNb27lh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Female', 'Male'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the unique values of the column \"gender\"\n",
    "coffeeData[\"Gender\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VOurxC7n27lj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., nan,  0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the unique values of the column 'Decision':\n",
    "coffeeData['Decision'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13DjAqVR27lm"
   },
   "source": [
    "We now want to replace `1.0` and `0.0` in the `Decision` column by `YES` and `NO` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZH3aTEt27lm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 702 entries, 0 to 701\n",
      "Data columns (total 9 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Age                      702 non-null    int64  \n",
      " 1   Gender                   702 non-null    object \n",
      " 2   num_coffeeBags_per_year  702 non-null    int64  \n",
      " 3   spent_last_week          702 non-null    int64  \n",
      " 4   spent_month              702 non-null    int64  \n",
      " 5   SlrAY                    702 non-null    int64  \n",
      " 6   Distance                 702 non-null    float64\n",
      " 7   Online                   702 non-null    int64  \n",
      " 8   Decision                 474 non-null    object \n",
      "dtypes: float64(1), int64(6), object(2)\n",
      "memory usage: 49.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Call replace() on the Decision column to replace 1.0 and 0.0 by 'Yes' and 'No'\n",
    "coffeeData[\"Decision\"] = coffeeData[\"Decision\"].replace(1.0, \"Yes\")\n",
    "coffeeData[\"Decision\"] = coffeeData[\"Decision\"].replace(0.0, \"No\")\n",
    "coffeeData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EfyWNmGu27lt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Yes', nan, 'No'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that our replacing those values with 'YES' and 'NO' worked, with unique()\n",
    "coffeeData[\"Decision\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uSpkk4Er27ly"
   },
   "source": [
    "## 2b. Train/test split\n",
    "To execute the train/test split properly, we need to do five things: \n",
    "1. Drop all rows with a null value in the `Decision` column, and save the result as NOPrediction: a dataset that will contain all known values for the decision \n",
    "2. Visualize the data using scatter and boxplots of several variables in the y-axis and the decision on the x-axis\n",
    "3. Get the subset of coffeeData with null values in the `Decision` column, and save that subset as Prediction\n",
    "4. Divide the NOPrediction subset into X and y, and then further divide those subsets into train and test subsets for X and y respectively\n",
    "5. Create dummy variables to deal with categorical inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UuNUdVC027ly"
   },
   "source": [
    "### 1. Drop all null values within the `Decision` column, and save the result as NoPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYEjdjQi27lz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     474\n",
       "unique      2\n",
       "top       Yes\n",
       "freq      303\n",
       "Name: Decision, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NoPrediction will contain all known values for the decision\n",
    "# Call dropna() on coffeeData, and store the result in a variable NOPrediction \n",
    "# Call describe() on the Decision column of NoPrediction after calling dropna() on coffeeData\n",
    "NOPrediction = coffeeData.dropna()\n",
    "NOPrediction[\"Decision\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3pppAJIZ27l1"
   },
   "source": [
    "### 2. Visualize the data using scatter and boxplots of several variables in the y-axis and the decision on the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GpJlIyNU27l2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploring our new NOPrediction dataset\n",
    "# Call boxplot() on our Seaborn object sns, and plug y=\"spent_today\", x= \"Decision\", data=NOPrediction\n",
    "# Don't forget to call plt.slow() after that \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the boxplot\n",
    "sns.boxplot(y=\"spent_last_week\", x=\"Decision\", data=NOPrediction)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ulje1R9b27l4"
   },
   "source": [
    "Can you admissibly conclude anything from this boxplot? Write your answer here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HWrGiTkW27l5"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `spent_last_month` for parameter `y`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create the scatterplot\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatterplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspent_last_month\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDistance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNOPrediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Show the plot\u001b[39;00m\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\relational.py:742\u001b[0m, in \u001b[0;36mscatterplot\u001b[1;34m(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatterplot\u001b[39m(\n\u001b[0;32m    733\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    734\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    739\u001b[0m ):\n\u001b[0;32m    741\u001b[0m     variables \u001b[38;5;241m=\u001b[39m _ScatterPlotter\u001b[38;5;241m.\u001b[39mget_semantics(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[1;32m--> 742\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_ScatterPlotter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    744\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_hue(palette\u001b[38;5;241m=\u001b[39mpalette, order\u001b[38;5;241m=\u001b[39mhue_order, norm\u001b[38;5;241m=\u001b[39mhue_norm)\n\u001b[0;32m    745\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_size(sizes\u001b[38;5;241m=\u001b[39msizes, order\u001b[38;5;241m=\u001b[39msize_order, norm\u001b[38;5;241m=\u001b[39msize_norm)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\relational.py:538\u001b[0m, in \u001b[0;36m_ScatterPlotter.__init__\u001b[1;34m(self, data, variables, legend)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, variables\u001b[38;5;241m=\u001b[39m{}, legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    530\u001b[0m \n\u001b[0;32m    531\u001b[0m     \u001b[38;5;66;03m# TODO this is messy, we want the mapping to be agnostic about\u001b[39;00m\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# the kind of plot to draw, but for the time being we need to set\u001b[39;00m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;66;03m# this information so the SizeMapping can use it\u001b[39;00m\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_size_range \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    535\u001b[0m         np\u001b[38;5;241m.\u001b[39mr_[\u001b[38;5;241m.5\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlines.markersize\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    536\u001b[0m     )\n\u001b[1;32m--> 538\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegend \u001b[38;5;241m=\u001b[39m legend\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:640\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_ordered \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[1;32m--> 640\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var, \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_semantic_mappings\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    643\u001b[0m \n\u001b[0;32m    644\u001b[0m     \u001b[38;5;66;03m# Create the mapping function\u001b[39;00m\n\u001b[0;32m    645\u001b[0m     map_func \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmap, plotter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:701\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 701\u001b[0m     plot_data, variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_variables_longform(\n\u001b[0;32m    702\u001b[0m         data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvariables,\n\u001b[0;32m    703\u001b[0m     )\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_data \u001b[38;5;241m=\u001b[39m plot_data\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables \u001b[38;5;241m=\u001b[39m variables\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:938\u001b[0m, in \u001b[0;36mVectorPlotter._assign_variables_longform\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[0;32m    934\u001b[0m \n\u001b[0;32m    935\u001b[0m     \u001b[38;5;66;03m# This looks like a column name but we don't know what it means!\u001b[39;00m\n\u001b[0;32m    937\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret value `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` for parameter `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m     \u001b[38;5;66;03m# Otherwise, assume the value is itself data\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \n\u001b[0;32m    944\u001b[0m     \u001b[38;5;66;03m# Raise when data object is present and a vector can't matched\u001b[39;00m\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, pd\u001b[38;5;241m.\u001b[39mSeries):\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret value `spent_last_month` for parameter `y`"
     ]
    }
   ],
   "source": [
    "# Call scatterplot() on our Seaborn object sns, and plug in y=\"spent_last_month\", x= \"distance\", hue = \"Decision\", data =NOPrediction.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the scatterplot\n",
    "sns.scatterplot(y=\"spent_last_month\", x=\"Distance\", hue=\"Decision\", data=NOPrediction)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `spent_last_month` for parameter `y`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create the scatterplot\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatterplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspent_last_month\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDistance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNOPrediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Show the plot\u001b[39;00m\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\relational.py:742\u001b[0m, in \u001b[0;36mscatterplot\u001b[1;34m(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatterplot\u001b[39m(\n\u001b[0;32m    733\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    734\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    739\u001b[0m ):\n\u001b[0;32m    741\u001b[0m     variables \u001b[38;5;241m=\u001b[39m _ScatterPlotter\u001b[38;5;241m.\u001b[39mget_semantics(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[1;32m--> 742\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_ScatterPlotter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    744\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_hue(palette\u001b[38;5;241m=\u001b[39mpalette, order\u001b[38;5;241m=\u001b[39mhue_order, norm\u001b[38;5;241m=\u001b[39mhue_norm)\n\u001b[0;32m    745\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_size(sizes\u001b[38;5;241m=\u001b[39msizes, order\u001b[38;5;241m=\u001b[39msize_order, norm\u001b[38;5;241m=\u001b[39msize_norm)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\relational.py:538\u001b[0m, in \u001b[0;36m_ScatterPlotter.__init__\u001b[1;34m(self, data, variables, legend)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, variables\u001b[38;5;241m=\u001b[39m{}, legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    530\u001b[0m \n\u001b[0;32m    531\u001b[0m     \u001b[38;5;66;03m# TODO this is messy, we want the mapping to be agnostic about\u001b[39;00m\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# the kind of plot to draw, but for the time being we need to set\u001b[39;00m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;66;03m# this information so the SizeMapping can use it\u001b[39;00m\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_size_range \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    535\u001b[0m         np\u001b[38;5;241m.\u001b[39mr_[\u001b[38;5;241m.5\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlines.markersize\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    536\u001b[0m     )\n\u001b[1;32m--> 538\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegend \u001b[38;5;241m=\u001b[39m legend\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:640\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_ordered \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[1;32m--> 640\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var, \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_semantic_mappings\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    643\u001b[0m \n\u001b[0;32m    644\u001b[0m     \u001b[38;5;66;03m# Create the mapping function\u001b[39;00m\n\u001b[0;32m    645\u001b[0m     map_func \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmap, plotter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:701\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 701\u001b[0m     plot_data, variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_variables_longform(\n\u001b[0;32m    702\u001b[0m         data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvariables,\n\u001b[0;32m    703\u001b[0m     )\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_data \u001b[38;5;241m=\u001b[39m plot_data\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables \u001b[38;5;241m=\u001b[39m variables\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:938\u001b[0m, in \u001b[0;36mVectorPlotter._assign_variables_longform\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[0;32m    934\u001b[0m \n\u001b[0;32m    935\u001b[0m     \u001b[38;5;66;03m# This looks like a column name but we don't know what it means!\u001b[39;00m\n\u001b[0;32m    937\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret value `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` for parameter `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m     \u001b[38;5;66;03m# Otherwise, assume the value is itself data\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \n\u001b[0;32m    944\u001b[0m     \u001b[38;5;66;03m# Raise when data object is present and a vector can't matched\u001b[39;00m\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, pd\u001b[38;5;241m.\u001b[39mSeries):\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret value `spent_last_month` for parameter `y`"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the scatterplot\n",
    "sns.scatterplot(y=\"spent_last_month\", x=\"Distance\", hue=\"Decision\", data=NOPrediction)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRNoE4Tb27l8"
   },
   "source": [
    "Can you admissibly conclude anything from this scatterplot? Remember: we are trying to build a tree to classify unseen examples. Write your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'Gender', 'num_coffeeBags_per_year', 'spent_last_week',\n",
      "       'spent_month', 'SlrAY', 'Distance', 'Online', 'Decision'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `SpentLastMonth` for parameter `y`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(NOPrediction\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Replace \"spent_last_month\" with the correct column name if needed\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming the correct column name is \"SpentLastMonth\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatterplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSpentLastMonth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDistance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNOPrediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Show the plot\u001b[39;00m\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\relational.py:742\u001b[0m, in \u001b[0;36mscatterplot\u001b[1;34m(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatterplot\u001b[39m(\n\u001b[0;32m    733\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    734\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    739\u001b[0m ):\n\u001b[0;32m    741\u001b[0m     variables \u001b[38;5;241m=\u001b[39m _ScatterPlotter\u001b[38;5;241m.\u001b[39mget_semantics(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[1;32m--> 742\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_ScatterPlotter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    744\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_hue(palette\u001b[38;5;241m=\u001b[39mpalette, order\u001b[38;5;241m=\u001b[39mhue_order, norm\u001b[38;5;241m=\u001b[39mhue_norm)\n\u001b[0;32m    745\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_size(sizes\u001b[38;5;241m=\u001b[39msizes, order\u001b[38;5;241m=\u001b[39msize_order, norm\u001b[38;5;241m=\u001b[39msize_norm)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\relational.py:538\u001b[0m, in \u001b[0;36m_ScatterPlotter.__init__\u001b[1;34m(self, data, variables, legend)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, variables\u001b[38;5;241m=\u001b[39m{}, legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    530\u001b[0m \n\u001b[0;32m    531\u001b[0m     \u001b[38;5;66;03m# TODO this is messy, we want the mapping to be agnostic about\u001b[39;00m\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# the kind of plot to draw, but for the time being we need to set\u001b[39;00m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;66;03m# this information so the SizeMapping can use it\u001b[39;00m\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_size_range \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    535\u001b[0m         np\u001b[38;5;241m.\u001b[39mr_[\u001b[38;5;241m.5\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlines.markersize\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    536\u001b[0m     )\n\u001b[1;32m--> 538\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegend \u001b[38;5;241m=\u001b[39m legend\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:640\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_ordered \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[1;32m--> 640\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var, \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_semantic_mappings\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    643\u001b[0m \n\u001b[0;32m    644\u001b[0m     \u001b[38;5;66;03m# Create the mapping function\u001b[39;00m\n\u001b[0;32m    645\u001b[0m     map_func \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmap, plotter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:701\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 701\u001b[0m     plot_data, variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_variables_longform(\n\u001b[0;32m    702\u001b[0m         data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvariables,\n\u001b[0;32m    703\u001b[0m     )\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_data \u001b[38;5;241m=\u001b[39m plot_data\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables \u001b[38;5;241m=\u001b[39m variables\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:938\u001b[0m, in \u001b[0;36mVectorPlotter._assign_variables_longform\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[0;32m    934\u001b[0m \n\u001b[0;32m    935\u001b[0m     \u001b[38;5;66;03m# This looks like a column name but we don't know what it means!\u001b[39;00m\n\u001b[0;32m    937\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret value `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` for parameter `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m     \u001b[38;5;66;03m# Otherwise, assume the value is itself data\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \n\u001b[0;32m    944\u001b[0m     \u001b[38;5;66;03m# Raise when data object is present and a vector can't matched\u001b[39;00m\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, pd\u001b[38;5;241m.\u001b[39mSeries):\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret value `SpentLastMonth` for parameter `y`"
     ]
    }
   ],
   "source": [
    "# Check the column names of NOPrediction\n",
    "print(NOPrediction.columns)\n",
    "\n",
    "# Replace \"spent_last_month\" with the correct column name if needed\n",
    "# Assuming the correct column name is \"SpentLastMonth\"\n",
    "sns.scatterplot(y=\"SpentLastMonth\", x=\"Distance\", hue=\"Decision\", data=NOPrediction)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XL6yGmQY27l8"
   },
   "source": [
    "### 3. Get the subset of coffeeData with null values in the Decision column, and save that subset as Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EO9pctLS27l9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>num_coffeeBags_per_year</th>\n",
       "      <th>spent_last_week</th>\n",
       "      <th>spent_month</th>\n",
       "      <th>SlrAY</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Online</th>\n",
       "      <th>Decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>164</td>\n",
       "      <td>74035</td>\n",
       "      <td>0.520906</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>107</td>\n",
       "      <td>13166</td>\n",
       "      <td>0.932098</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>17425</td>\n",
       "      <td>1.193188</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>153</td>\n",
       "      <td>84803</td>\n",
       "      <td>1.655096</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>122</td>\n",
       "      <td>42338</td>\n",
       "      <td>1.714179</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Gender  num_coffeeBags_per_year  spent_last_week  spent_month  SlrAY  \\\n",
       "1    24    Male                        0               44          164  74035   \n",
       "3    20    Male                        0               30          107  13166   \n",
       "7    24  Female                        0               20           34  17425   \n",
       "11   24  Female                        0               40          153  84803   \n",
       "12   21  Female                        0               38          122  42338   \n",
       "\n",
       "    Distance  Online Decision  \n",
       "1   0.520906       0      NaN  \n",
       "3   0.932098       1      NaN  \n",
       "7   1.193188       0      NaN  \n",
       "11  1.655096       1      NaN  \n",
       "12  1.714179       1      NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get just those rows whose value for the Decision column is null. There are lots of ways to do this.\n",
    "# One way is to subset on pd.isnull(data['Decision']). Use square brackets, and plug that in as parameter.\n",
    "# Store the result in a variable called Prediction \n",
    "# Call a head() on the result to see it's worked out alright \n",
    "Prediction = coffeeData[pd.isnull(coffeeData[\"Decision\"])]\n",
    "Prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LAEU5mem27l_"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>num_coffeeBags_per_year</th>\n",
       "      <th>spent_last_week</th>\n",
       "      <th>spent_month</th>\n",
       "      <th>SlrAY</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Online</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>228.000000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>228.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31.802632</td>\n",
       "      <td>2.960526</td>\n",
       "      <td>33.394737</td>\n",
       "      <td>110.407895</td>\n",
       "      <td>41923.741228</td>\n",
       "      <td>3.428836</td>\n",
       "      <td>0.570175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.302293</td>\n",
       "      <td>1.585514</td>\n",
       "      <td>15.697930</td>\n",
       "      <td>53.786536</td>\n",
       "      <td>27406.768360</td>\n",
       "      <td>2.153102</td>\n",
       "      <td>0.496140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1617.000000</td>\n",
       "      <td>0.010048</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>15911.500000</td>\n",
       "      <td>1.699408</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>40987.500000</td>\n",
       "      <td>3.208673</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>151.250000</td>\n",
       "      <td>58537.000000</td>\n",
       "      <td>5.261184</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>182058.000000</td>\n",
       "      <td>10.871566</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Age  num_coffeeBags_per_year  spent_last_week  spent_month  \\\n",
       "count  228.000000               228.000000       228.000000   228.000000   \n",
       "mean    31.802632                 2.960526        33.394737   110.407895   \n",
       "std     14.302293                 1.585514        15.697930    53.786536   \n",
       "min     16.000000                 0.000000         0.000000     0.000000   \n",
       "25%     22.000000                 2.000000        25.750000    65.000000   \n",
       "50%     25.000000                 3.000000        37.000000   113.500000   \n",
       "75%     39.000000                 4.000000        44.000000   151.250000   \n",
       "max     67.000000                 5.000000        62.000000   210.000000   \n",
       "\n",
       "               SlrAY    Distance      Online  \n",
       "count     228.000000  228.000000  228.000000  \n",
       "mean    41923.741228    3.428836    0.570175  \n",
       "std     27406.768360    2.153102    0.496140  \n",
       "min      1617.000000    0.010048    0.000000  \n",
       "25%     15911.500000    1.699408    0.000000  \n",
       "50%     40987.500000    3.208673    1.000000  \n",
       "75%     58537.000000    5.261184    1.000000  \n",
       "max    182058.000000   10.871566    1.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call describe() on Prediction\n",
    "Prediction.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c3HLaOFk27mB"
   },
   "source": [
    "### 4. Divide the NOPrediction subset into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkD9WISM27mC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Gender', 'num_coffeeBags_per_year', 'spent_last_week',\n",
       "       'spent_month', 'SlrAY', 'Distance', 'Online', 'Decision'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First of all, let's check the names of the columns of NOPrediction\n",
    "NOPrediction.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O8ndj3ZB27mE"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['spent_last_month', 'Salary'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_coffeeBags_per_year\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspent_last_week\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspent_last_month\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistance\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnline\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Explanatory variable (X)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mNOPrediction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Dependent variable (y)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m y \u001b[38;5;241m=\u001b[39m NOPrediction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecision\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6070\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['spent_last_month', 'Salary'] not in index\""
     ]
    }
   ],
   "source": [
    "# Let's do our feature selection.\n",
    "# Make a variable called 'features', and a list containing the strings of every column except \"Decision\"; that is:\n",
    "# [\"age\", \"gender\", \"num_coffeeBags_per_year\", \"spent_last_week\", \"spent_last_month\", \"Salary\", \"Distance\", \"Online\"]\n",
    "# List of features\n",
    "features = [\"Age\", \"Gender\", \"num_coffeeBags_per_year\", \"spent_last_week\", \"spent_last_month\", \"Salary\", \"Distance\", \"Online\"]\n",
    "\n",
    "# Explanatory variable (X)\n",
    "X = NOPrediction[features]\n",
    "\n",
    "# Dependent variable (y)\n",
    "y = NOPrediction[\"Decision\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features\n",
    "features = [\"Age\", \"Gender\", \"num_coffeeBags_per_year\", \"spent_last_week\", \"Distance\", \"Online\"]\n",
    "\n",
    "# Explanatory variable (X)\n",
    "X = NOPrediction[features]\n",
    "\n",
    "# Dependent variable (y)\n",
    "y = NOPrediction[\"Decision\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dzZUinX-27mK"
   },
   "source": [
    "### 5. Create dummy variables to deal with categorical inputs\n",
    "One-hot encoding replaces each unique value of a given column with a new column, and puts a 1 in the new column for a given row just if its initial value for the original column matches the new column. Check out [this resource](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) if you haven't seen one-hot-encoding before. \n",
    "\n",
    "**Note**: We will do this before we do our train/test split as to do it after could mean that some categories only end up in the train or test split of our data by chance and this would then lead to different shapes of data for our `X_train` and `X_test` which could/would cause downstream issues when fitting or predicting using a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZTpJpBX27mL"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# One-hot encoding all features in training set.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Call get_dummies() on our Pandas objet pd, and pass X to it. Reassign the result back to X. \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(\u001b[43mX\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# One-hot encoding all features in training set.\n",
    "# Call get_dummies() on our Pandas objet pd, and pass X to it. Reassign the result back to X. \n",
    "X = pd.get_dummies(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X before calling pd.get_dummies(X)\n",
    "X = NOPrediction[features]\n",
    "\n",
    "# One-hot encode all features in the training set\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GdigaqRI27mG"
   },
   "source": [
    "### 6. Further divide those subsets into train and test subsets for X and y respectively: X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E8aJs3PL27mG"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=246)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cp0ibTlP27mN"
   },
   "source": [
    "# 3. Modelling\n",
    "It's useful to look at the scikit-learn documentation on decision trees https://scikit-learn.org/stable/modules/tree.html before launching into applying them. If you haven't seen them before, take a look at that link, in particular the section `1.10.5.` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4TxsvBr-27mN"
   },
   "source": [
    "## Model 1: Entropy model - no max_depth\n",
    "\n",
    "We'll give you a little more guidance here, as the Python is hard to deduce, and scikitlearn takes some getting used to.\n",
    "\n",
    "Theoretically, let's remind ourselves of what's going on with a decision tree implementing an entropy model.\n",
    "\n",
    "Ross Quinlan's **ID3 Algorithm** was one of the first, and one of the most basic, to use entropy as a metric.\n",
    "\n",
    "**Entropy** is a measure of how uncertain we are about which category the data-points fall into at a given point in the tree. The **Information gain** of a specific feature with a threshold (such as 'spent_last_month <= 138.0') is the difference in entropy that exists before and after splitting on that feature; i.e., the information we gain about the categories of the data-points by splitting on that feature and that threshold. \n",
    "\n",
    "Naturally, we want to minimize entropy and maximize information gain. Quinlan's ID3 algorithm is designed to output a tree such that the features at each node, starting from the root, and going all the way down to the leaves, have maximial information gain. We want a tree whose leaves have elements that are *homogeneous*, that is, all of the same category. \n",
    "\n",
    "The first model will be the hardest. Persevere and you'll reap the rewards: you can use almost exactly the same code for the other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dn_gV-_k27mO"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, random_state=1234)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, random_state=1234)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=1234)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Declare decision tree classifier\n",
    "entr_model = DecisionTreeClassifier(criterion=\"entropy\", random_state=1234)\n",
    "\n",
    "# Fit the model\n",
    "entr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = entr_model.predict(X_test)\n",
    "\n",
    "# Convert y_pred to a pandas Series\n",
    "y_pred = pd.Series(y_pred)\n",
    "\n",
    "# Check the model\n",
    "entr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bk61p9Bu27mQ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 13\u001b[0m\n\u001b[0;32m      8\u001b[0m dot_data \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Export the decision tree to dot format\u001b[39;00m\n\u001b[0;32m     11\u001b[0m export_graphviz(entr_model, out_file\u001b[38;5;241m=\u001b[39mdot_data,  \n\u001b[0;32m     12\u001b[0m                 filled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rounded\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m---> 13\u001b[0m                 special_characters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, feature_names\u001b[38;5;241m=\u001b[39m\u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns, class_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Alternatively, you can use the classes_ attribute of entr_model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# export_graphviz(entr_model, out_file=dot_data,  \u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#                 filled=True, rounded=True,\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#                 special_characters=True, feature_names=X_train.columns, class_names=entr_model.classes_)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Create the graph from dot data\u001b[39;00m\n\u001b[0;32m     21\u001b[0m graph \u001b[38;5;241m=\u001b[39m pydotplus\u001b[38;5;241m.\u001b[39mgraph_from_dot_data(dot_data\u001b[38;5;241m.\u001b[39mgetvalue())  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Now we want to visualize the tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from io import StringIO\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "\n",
    "# Generate dot data\n",
    "dot_data = StringIO()\n",
    "\n",
    "# Export the decision tree to dot format\n",
    "export_graphviz(entr_model, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True, feature_names=X_train.columns, class_names=[\"NO\", \"YES\"])\n",
    "\n",
    "# Alternatively, you can use the classes_ attribute of entr_model\n",
    "# export_graphviz(entr_model, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True, feature_names=X_train.columns, class_names=entr_model.classes_)\n",
    "\n",
    "# Create the graph from dot data\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "\n",
    "# Display the decision tree\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvocationException",
     "evalue": "GraphViz's executables not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvocationException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m graph \u001b[38;5;241m=\u001b[39m pydotplus\u001b[38;5;241m.\u001b[39mgraph_from_dot_data(dot_data\u001b[38;5;241m.\u001b[39mgetvalue())  \n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Display the decision tree\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m Image(\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydotplus\\graphviz.py:1797\u001b[0m, in \u001b[0;36mDot.__init__.<locals>.<lambda>\u001b[1;34m(f, prog)\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;66;03m# Automatically creates all the methods enabling the creation\u001b[39;00m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;66;03m# of output in any of the supported formats.\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frmt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformats:\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m frmt,\n\u001b[1;32m-> 1797\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m f\u001b[38;5;241m=\u001b[39mfrmt, prog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1798\u001b[0m     )\n\u001b[0;32m   1799\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m frmt]\n\u001b[0;32m   1800\u001b[0m     f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;124;03m'''Refer to the docstring accompanying the'''\u001b[39;00m\n\u001b[0;32m   1802\u001b[0m         \u001b[38;5;124;03m''''create' method for more information.'''\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydotplus\\graphviz.py:1959\u001b[0m, in \u001b[0;36mDot.create\u001b[1;34m(self, prog, format)\u001b[0m\n\u001b[0;32m   1957\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogs \u001b[38;5;241m=\u001b[39m find_graphviz()\n\u001b[0;32m   1958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1959\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvocationException(\n\u001b[0;32m   1960\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGraphViz\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms executables not found\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prog \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogs:\n\u001b[0;32m   1963\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvocationException(\n\u001b[0;32m   1964\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGraphViz\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms executable \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m prog)\n",
      "\u001b[1;31mInvocationException\u001b[0m: GraphViz's executables not found"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from io import StringIO\n",
    "from IPython.display import Image\n",
    "\n",
    "# Assuming X_train is defined earlier in your code\n",
    "\n",
    "# Generate dot data\n",
    "dot_data = StringIO()\n",
    "\n",
    "# Export the decision tree to dot format\n",
    "export_graphviz(entr_model, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True, feature_names=X_train.columns, class_names=[\"NO\", \"YES\"])\n",
    "\n",
    "# Alternatively, you can use the classes_ attribute of entr_model\n",
    "# export_graphviz(entr_model, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True, feature_names=X_train.columns, class_names=entr_model.classes_)\n",
    "\n",
    "# Create the graph from dot data\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "\n",
    "# Display the decision tree\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G4KbEX0827mV"
   },
   "source": [
    "## Model 1: Entropy model - no max_depth: Interpretation and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O7wTOJqo27mW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Entropy - no max depth\n",
      "Accuracy: 0.8571428571428571\n",
      "Balanced accuracy: 0.8505315822388992\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pos_label=YES is not a valid label. It should be one of ['No', 'Yes']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m.\u001b[39maccuracy_score(y_test,y_pred))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBalanced accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m.\u001b[39mbalanced_accuracy_score(y_test,y_pred))\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision score for \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYES\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision score for \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m , metrics\u001b[38;5;241m.\u001b[39mprecision_score(y_test,y_pred, pos_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall score for \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m , metrics\u001b[38;5;241m.\u001b[39mrecall_score(y_test,y_pred, pos_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2127\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1970\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1971\u001b[0m     {\n\u001b[0;32m   1972\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1996\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1997\u001b[0m ):\n\u001b[0;32m   1998\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   1999\u001b[0m \n\u001b[0;32m   2000\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2125\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   2126\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2127\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2129\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2132\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2134\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    186\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1721\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1563\u001b[0m \u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[0;32m   1564\u001b[0m \n\u001b[0;32m   1565\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[0;32m   1719\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1720\u001b[0m zero_division_value \u001b[38;5;241m=\u001b[39m _check_zero_division(zero_division)\n\u001b[1;32m-> 1721\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1724\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1507\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pos_label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m present_labels:\n\u001b[0;32m   1506\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(present_labels) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1507\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1508\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_label=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid label. It \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1509\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpresent_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1510\u001b[0m             )\n\u001b[0;32m   1511\u001b[0m     labels \u001b[38;5;241m=\u001b[39m [pos_label]\n\u001b[0;32m   1512\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: pos_label=YES is not a valid label. It should be one of ['No', 'Yes']"
     ]
    }
   ],
   "source": [
    "# Run this block for model evaluation metrics \n",
    "print(\"Model Entropy - no max depth\")\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
    "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
    "print('Precision score for \"Yes\"' , metrics.precision_score(y_test,y_pred, pos_label = \"YES\"))\n",
    "print('Precision score for \"No\"' , metrics.precision_score(y_test,y_pred, pos_label = \"NO\"))\n",
    "print('Recall score for \"Yes\"' , metrics.recall_score(y_test,y_pred, pos_label = \"YES\"))\n",
    "print('Recall score for \"No\"' , metrics.recall_score(y_test,y_pred, pos_label = \"NO\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Entropy - no max depth\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Run this block for model evaluation metrics \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Entropy - no max depth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m.\u001b[39maccuracy_score(\u001b[43my_test\u001b[49m, y_pred))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBalanced accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m.\u001b[39mbalanced_accuracy_score(y_test, y_pred))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision score for \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m.\u001b[39mprecision_score(y_test, y_pred, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Run this block for model evaluation metrics \n",
    "print(\"Model Entropy - no max depth\")\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test, y_pred))\n",
    "print('Precision score for \"Yes\"', metrics.precision_score(y_test, y_pred, pos_label=\"YES\"))\n",
    "print('Precision score for \"No\"', metrics.precision_score(y_test, y_pred, pos_label=\"NO\"))\n",
    "print('Recall score for \"Yes\"', metrics.recall_score(y_test, y_pred, pos_label=\"YES\"))\n",
    "print('Recall score for \"No\"', metrics.recall_score(y_test, y_pred, pos_label=\"NO\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptzUYFpt27mY"
   },
   "source": [
    "What can you infer from these results? Write your conclusions here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oe4kkpM027ma"
   },
   "source": [
    "## Model 2: Gini impurity model - no max_depth\n",
    "\n",
    "Gini impurity, like entropy, is a measure of how well a given feature (and threshold) splits the data into categories.\n",
    "\n",
    "Their equations are similar, but Gini impurity doesn't require logorathmic functions, which can be computationally expensive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9d4WO4lt27ma"
   },
   "outputs": [],
   "source": [
    "# Make a variable called gini_model, and assign it exactly what you assigned entr_model with above, but with the\n",
    "# criterion changed to 'gini'\n",
    "_ _ _ = tree.DecisionTreeClassifier(_ _ _=\"_ _ _\", _ _ _ = 1234)\n",
    "\n",
    "# Call fit() on the gini_model as you did with the entr_model\n",
    "gini_model._ _ _(_ _ _, y_train)\n",
    "\n",
    "# Call predict() on the gini_model as you did with the entr_model \n",
    "y_pred = _ _ _.predict(X_test)\n",
    "\n",
    "# Turn y_pred into a series, as before\n",
    "y_pred = pd.Series(_ _ _)\n",
    "\n",
    "# Check out gini_model\n",
    "gini_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make a variable called gini_model, and assign it exactly what you assigned entr_model with above, but with the\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# criterion changed to 'gini'\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m gini_model \u001b[38;5;241m=\u001b[39m \u001b[43mtree\u001b[49m\u001b[38;5;241m.\u001b[39mDecisionTreeClassifier(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Call fit() on the gini_model as you did with the entr_model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m gini_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tree' is not defined"
     ]
    }
   ],
   "source": [
    "# Make a variable called gini_model, and assign it exactly what you assigned entr_model with above, but with the\n",
    "# criterion changed to 'gini'\n",
    "gini_model = tree.DecisionTreeClassifier(criterion='gini', random_state=1234)\n",
    "\n",
    "# Call fit() on the gini_model as you did with the entr_model\n",
    "gini_model.fit(X_train, y_train)\n",
    "\n",
    "# Call predict() on the gini_model as you did with the entr_model \n",
    "y_pred = gini_model.predict(X_test)\n",
    "\n",
    "# Turn y_pred into a series, as before\n",
    "y_pred = pd.Series(y_pred)\n",
    "\n",
    "# Check out gini_model\n",
    "gini_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdWfcr7n27mc"
   },
   "outputs": [],
   "source": [
    "# As before, but make the model name gini_model\n",
    "dot_data = StringIO()\n",
    "tree._ _ _ (_ _ _ , out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True, feature_names=X_train.columns,class_names = [\"NO\", \"YES\"])\n",
    "\n",
    "# Alternatively for class_names use gini_model.classes_\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# As before, but make the model name gini_model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dot_data \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241m.\u001b[39mexport_graphviz(gini_model, out_file\u001b[38;5;241m=\u001b[39mdot_data,  \n\u001b[0;32m      4\u001b[0m                 filled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rounded\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m                 special_characters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, feature_names\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mcolumns,class_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Alternatively for class_names use gini_model.classes_\u001b[39;00m\n\u001b[0;32m      8\u001b[0m graph \u001b[38;5;241m=\u001b[39m pydotplus\u001b[38;5;241m.\u001b[39mgraph_from_dot_data(dot_data\u001b[38;5;241m.\u001b[39mgetvalue())  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'tree' is not defined"
     ]
    }
   ],
   "source": [
    "# As before, but make the model name gini_model\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(gini_model, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True, feature_names=X_train.columns,class_names = [\"NO\", \"YES\"])\n",
    "\n",
    "# Alternatively for class_names use gini_model.classes_\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yCgKCQEM27me"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Gini impurity model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run this block for model evaluation\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Gini impurity model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m.\u001b[39maccuracy_score(\u001b[43my_test\u001b[49m,y_pred))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBalanced accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m.\u001b[39mbalanced_accuracy_score(y_test,y_pred))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision score\u001b[39m\u001b[38;5;124m'\u001b[39m , metrics\u001b[38;5;241m.\u001b[39mprecision_score(y_test,y_pred, pos_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Run this block for model evaluation\n",
    "print(\"Model Gini impurity model\")\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
    "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
    "print('Precision score' , metrics.precision_score(y_test,y_pred, pos_label = \"YES\"))\n",
    "print('Recall score' , metrics.recall_score(y_test,y_pred, pos_label = \"NO\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FgCPZf0c27mh"
   },
   "source": [
    "How do the results here compare to the previous model? Write your judgements here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88xuZI4927mi"
   },
   "source": [
    "## Model 3: Entropy model - max depth 3\n",
    "We're going to try to limit the depth of our decision tree, using entropy first.  \n",
    "\n",
    "As you know, we need to strike a balance with tree depth. \n",
    "\n",
    "Insufficiently deep, and we're not giving the tree the opportunity to spot the right patterns in the training data.\n",
    "\n",
    "Excessively deep, and we're probably going to make a tree that overfits to the training data, at the cost of very high error on the (hitherto unseen) test data. \n",
    "\n",
    "Sophisticated data scientists use methods like random search with cross-validation to systematically find a good depth for their tree. We'll start with picking 3, and see how that goes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sIqsFSm127mj"
   },
   "outputs": [],
   "source": [
    "# Made a model as before, but call it entr_model2, and make the max_depth parameter equal to 3. \n",
    "# Execute the fitting, predicting, and Series operations as before\n",
    "_ _ _ = _ _ _._ _ _(criterion=\"entropy\", max_depth = _ _ _, random_state = 1234)\n",
    "entr_model2._ _ _(X_train, y_train)\n",
    "y_pred = entr_model2._ _ _(X_test)\n",
    "y_pred = pd._ _ _(y_pred)\n",
    "entr_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m entr_model2 \u001b[38;5;241m=\u001b[39m \u001b[43mtree\u001b[49m\u001b[38;5;241m.\u001b[39mDecisionTreeClassifier(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m)\n\u001b[0;32m      2\u001b[0m entr_model2\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m entr_model2\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tree' is not defined"
     ]
    }
   ],
   "source": [
    "entr_model2 = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, random_state=1234)\n",
    "entr_model2.fit(X_train, y_train)\n",
    "y_pred = entr_model2.predict(X_test)\n",
    "y_pred = pd.Series(y_pred)\n",
    "entr_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jQMihcyY27ml"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# As before, we need to visualize the tree to grasp its nature\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dot_data \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241m.\u001b[39mexport_graphviz(entr_model2, out_file\u001b[38;5;241m=\u001b[39mdot_data,  \n\u001b[0;32m      4\u001b[0m                 filled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rounded\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m                 special_characters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, feature_names\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mcolumns,class_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Alternatively for class_names use entr_model2.classes_\u001b[39;00m\n\u001b[0;32m      8\u001b[0m graph \u001b[38;5;241m=\u001b[39m pydotplus\u001b[38;5;241m.\u001b[39mgraph_from_dot_data(dot_data\u001b[38;5;241m.\u001b[39mgetvalue())  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'tree' is not defined"
     ]
    }
   ],
   "source": [
    "# As before, we need to visualize the tree to grasp its nature\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(entr_model2, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True, feature_names=X_train.columns,class_names = [\"NO\", \"YES\"])\n",
    "\n",
    "# Alternatively for class_names use entr_model2.classes_\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUnwNy3w27mn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Entropy model max depth 3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run this block for model evaluation \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Entropy model max depth 3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m.\u001b[39maccuracy_score(\u001b[43my_test\u001b[49m,y_pred))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBalanced accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m.\u001b[39mbalanced_accuracy_score(y_test,y_pred))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision score for \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m , metrics\u001b[38;5;241m.\u001b[39mprecision_score(y_test,y_pred, pos_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Run this block for model evaluation \n",
    "print(\"Model Entropy model max depth 3\")\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
    "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
    "print('Precision score for \"Yes\"' , metrics.precision_score(y_test,y_pred, pos_label = \"YES\"))\n",
    "print('Recall score for \"No\"' , metrics.recall_score(y_test,y_pred, pos_label = \"NO\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5U22BCW27mv"
   },
   "source": [
    "So our accuracy decreased, but is this certainly an inferior tree to the max depth original tree we did with Model 1? Write your conclusions here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iy9DmgIG27mw"
   },
   "source": [
    "## Model 4: Gini impurity  model - max depth 3\n",
    "We're now going to try the same with the Gini impurity model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QB-UrxQv27mw"
   },
   "outputs": [],
   "source": [
    "# As before, make a variable, but call it gini_model2, and ensure the max_depth parameter is set to 3\n",
    "_ _ _ = _ _ _._ _ _(criterion='gini', _ _ _ = 1234, max_depth = 3)\n",
    "\n",
    "# Do the fit, predict, and series transformations as before. \n",
    "gini_model2._ _ _(X_train, y_train)\n",
    "y_pred = gini_model2._ _ _(X_test)\n",
    "_ _ _ = pd.Series(y_pred)\n",
    "gini_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gini_model2 \u001b[38;5;241m=\u001b[39m \u001b[43mtree\u001b[49m\u001b[38;5;241m.\u001b[39mDecisionTreeClassifier(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      2\u001b[0m gini_model2\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m gini_model2\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tree' is not defined"
     ]
    }
   ],
   "source": [
    "gini_model2 = tree.DecisionTreeClassifier(criterion='gini', random_state=1234, max_depth=3)\n",
    "gini_model2.fit(X_train, y_train)\n",
    "y_pred = gini_model2.predict(X_test)\n",
    "y_pred = pd.Series(y_pred)\n",
    "gini_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgAGJ8Aw27mz"
   },
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "tree._ _ _(gini_model2, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True, feature_names=X_train.columns,class_names = [\"NO\", \"YES\"])\n",
    "\n",
    "# Alternatively for class_names use gini_model2.classes_\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKNGp4-127m0"
   },
   "outputs": [],
   "source": [
    "print(\"Gini impurity  model - max depth 3\")\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
    "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
    "print('Precision score' , metrics.precision_score(y_test,y_pred, pos_label = \"YES\"))\n",
    "print('Recall score' , metrics.recall_score(y_test,y_pred, pos_label = \"NO\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCgp_QEh27m2"
   },
   "source": [
    "Now this is an elegant tree. Its accuracy might not be the highest, but it's still the best model we've produced so far. Why is that? Write your answer here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XNwLDdpq27m3"
   },
   "source": [
    "# 4. Evaluating and concluding\n",
    "## 4a. How many customers will buy Hidden Farm coffee? \n",
    "Let's first ascertain how many loyal customers claimed, in the survey, that they will purchase the Hidden Farm coffee. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pxCzly1T27m7"
   },
   "outputs": [],
   "source": [
    "# Call value_counts() on the 'Decision' column of the original coffeeData\n",
    "coffeeData[\"Decision\"]._ _ _()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes    303\n",
       "No     171\n",
       "Name: Decision, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffeeData[\"Decision\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a01RQxef27m9"
   },
   "source": [
    "Let's now determine the number of people that, according to the model, will be willing to buy the Hidden Farm coffee. \n",
    "1. First we subset the Prediction dataset into `new_X` considering all the variables except `Decision` \n",
    "2. Use that dataset to predict a new variable called `potential_buyers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sN4Z56Sb27m9"
   },
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "# Make a variable called feature_cols, and assign it a list containing all the column names except 'Decision'\n",
    "_ _ _ = [\"Age\", \"Gender\", \"num_coffeeBags_per_year\", \"spent_last_week\", \"spent_last_month\",\n",
    "       \"Salary\", \"Distance\", \"Online\"]\n",
    "\n",
    "# Make a variable called new_X, and assign it the subset of Prediction, containing just the feature_cols \n",
    "new_X = Prediction[_ _ _]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['spent_last_month', 'Salary'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m feature_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_coffeeBags_per_year\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspent_last_week\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspent_last_month\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistance\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnline\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Make a variable called new_X, and assign it the subset of Prediction, containing just the feature_cols \u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m new_X \u001b[38;5;241m=\u001b[39m \u001b[43mPrediction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6070\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['spent_last_month', 'Salary'] not in index\""
     ]
    }
   ],
   "source": [
    "# Make a variable called feature_cols, and assign it a list containing all the column names except 'Decision'\n",
    "feature_cols = [\"Age\", \"Gender\", \"num_coffeeBags_per_year\", \"spent_last_week\", \"spent_last_month\",\n",
    "                \"Salary\", \"Distance\", \"Online\"]\n",
    "\n",
    "# Make a variable called new_X, and assign it the subset of Prediction, containing just the feature_cols \n",
    "new_X = Prediction[feature_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJtAgLwI27m_"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1668241755.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[56], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    potential_buyers = gini_model2._ _ _ (new_X)\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Call get_dummies() on the Pandas object pd, with new_X plugged in, to one-hot encode all features in the training set\n",
    "new_X = pd.get_dummies(new_X)\n",
    "\n",
    "# Make a variable called potential_buyers, and assign it the result of calling predict() on a model of your choice; \n",
    "# don't forget to pass new_X to predict()\n",
    "potential_buyers = gini_model2._ _ _ (new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Call get_dummies() on the Pandas object pd, with new_X plugged in, to one-hot encode all features in the training set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m new_X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(\u001b[43mnew_X\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Make a variable called potential_buyers, and assign it the result of calling predict() on a model of your choice; \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# don't forget to pass new_X to predict()\u001b[39;00m\n\u001b[0;32m      6\u001b[0m potential_buyers \u001b[38;5;241m=\u001b[39m gini_model2\u001b[38;5;241m.\u001b[39mpredict(new_X)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_X' is not defined"
     ]
    }
   ],
   "source": [
    "# Call get_dummies() on the Pandas object pd, with new_X plugged in, to one-hot encode all features in the training set\n",
    "new_X = pd.get_dummies(new_X)\n",
    "\n",
    "# Make a variable called potential_buyers, and assign it the result of calling predict() on a model of your choice; \n",
    "# don't forget to pass new_X to predict()\n",
    "potential_buyers = gini_model2.predict(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRHlWr6227nB"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Let's get the numbers of YES's and NO's in the potential buyers \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Call unique() on np, and pass potential_buyers and return_counts=True \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39munique(potential_buyers, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's get the numbers of YES's and NO's in the potential buyers \n",
    "# Call unique() on np, and pass potential_buyers and return_counts=True \n",
    "np.unique(potential_buyers, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80ZWqWGs27nD"
   },
   "source": [
    "The total number of potential buyers is 303 + 183 = 486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lMCYZ27Y27nD"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Salary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Print the total number of surveyed people \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe total number of surveyed people was\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mcoffeeData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSalary\u001b[49m\u001b[38;5;241m.\u001b[39mcount())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5896\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5899\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5900\u001b[0m ):\n\u001b[0;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Salary'"
     ]
    }
   ],
   "source": [
    "# Print the total number of surveyed people \n",
    "print(\"The total number of surveyed people was\", coffeeData.Salary.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oYM3t5JF27nF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6923076923076923"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's calculate the proportion of buyers\n",
    "486/702"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3jJDAZaC27nH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only  69.23 % of people want to buy the Hidden Farm coffee.\n"
     ]
    }
   ],
   "source": [
    "# Print the percentage of people who want to buy the Hidden Farm coffee, by our model \n",
    "print(\"Only \", round((486/702)*100, 2), \"% of people want to buy the Hidden Farm coffee.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kZwXNtjh27nK"
   },
   "source": [
    "## 4b. Decision\n",
    "Remember how you thought at the start: if more than 70% of the interviewed customers are likely to buy the Hidden Farm coffee, you will strike the deal with the local Hidden Farm farmers and sell the coffee. Otherwise, you won't strike the deal and the Hidden Farm coffee will remain in legends only. Well now's crunch time. Are you going to go ahead with that idea? If so, you won't be striking the deal with the Chinese farmers. \n",
    "\n",
    "They're called `decision trees`, aren't they? So where's the decision? What should you do? (Cue existential cat emoji). \n",
    "\n",
    "Ultimately, though, we can't write an algorithm to actually *make the business decision* for us. This is because such decisions depend on our values, what risks we are willing to take, the stakes of our decisions, and how important it us for us to *know* that we will succeed. What are you going to do with the models you've made? Are you going to risk everything, strike the deal with the *Hidden Farm* farmers, and sell the coffee? \n",
    "\n",
    "The philosopher of language Jason Stanley once wrote that the number of doubts our evidence has to rule out in order for us to know a given proposition depends on our stakes: the higher our stakes, the more doubts our evidence has to rule out, and therefore the harder it is for us to know things. We can end up paralyzed in predicaments; sometimes, we can act to better our situation only if we already know certain things, which we can only if our stakes were lower and we'd *already* bettered our situation. \n",
    "\n",
    "Data science and machine learning can't solve such problems. But what it can do is help us make great use of our data to help *inform* our decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crWWrpdox8i-"
   },
   "source": [
    "## 5. Random Forest\n",
    "You might have noticed an important fact about decision trees. Each time we run a given decision tree algorithm to make a prediction (such as whether customers will buy the Hidden Farm coffee) we will actually get a slightly different result. This might seem weird, but it has a simple explanation: machine learning algorithms are by definition ***stochastic***, in that their output is at least partly determined by randomness. \n",
    "\n",
    "To account for this variability and ensure that we get the most accurate prediction, we might want to actually make lots of decision trees, and get a value that captures the centre or average of the outputs of those trees. Luckily, there's a method for this, known as the ***Random Forest***. \n",
    "\n",
    "Essentially, Random Forest involves making lots of trees with similar properties, and then performing summary statistics on the outputs of those trees to reach that central value. Random forests are hugely powerful classifers, and they can improve predictive accuracy and control over-fitting. \n",
    "\n",
    "Why not try to inform your decision with random forest? You'll need to make use of the RandomForestClassifier function within the sklearn.ensemble module, found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fI4fiVWq0IH9"
   },
   "source": [
    "### 5a. Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vSRTnHnD0D-O"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jKmFvvjb0WB9"
   },
   "source": [
    "### 5b. Model\n",
    "You'll use your X_train and y_train variables just as before.\n",
    "\n",
    "You'll then need to make a variable (call it firstRFModel) to store your new Random Forest model. You'll assign this variable the result of calling RandomForestClassifier().\n",
    "\n",
    "Then, just as before, you'll call fit() on that firstRFModel variable, and plug in X_train and y_train.\n",
    "\n",
    "Finally, you should make a variable called y_pred, and assign it the result of calling the predict() method on your new firstRFModel, with the X_test data passed to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQEeTiRG1aSm"
   },
   "outputs": [],
   "source": [
    "# Plug in appropriate max_depth and random_state parameters \n",
    "firstRFModel = _ _ _(max_depth= _ _ _, random_state= _ _ _)\n",
    "firstRFModel._ _ _(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plug in appropriate max_depth and random_state parameters \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m firstRFModel \u001b[38;5;241m=\u001b[39m \u001b[43mRandomForestClassifier\u001b[49m(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m firstRFModel\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Plug in appropriate max_depth and random_state parameters \n",
    "firstRFModel = RandomForestClassifier(max_depth=10, random_state=42)\n",
    "firstRFModel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sCIt6pyn1zpb"
   },
   "source": [
    "### 5c. Revise conclusion\n",
    "\n",
    "Has your conclusion changed? Or is the result of executing random forest the same as your best model reached by a single decision tree? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Only 69.23% of customers wish to buy Hidden Farms coffee- this is not enough customers and lacks the confidence to take the high risk of purchasing coffee from the Hidden Farms coffee ranch. \n",
    "The analysis is the there is not enough confidence to take such a risk to invest toward such a venture."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "uSpkk4Er27ly",
    "UuNUdVC027ly",
    "3pppAJIZ27l1",
    "XL6yGmQY27l8",
    "c3HLaOFk27mB",
    "GdigaqRI27mG",
    "dzZUinX-27mK",
    "4TxsvBr-27mN",
    "G4KbEX0827mV",
    "oe4kkpM027ma",
    "88xuZI4927mi",
    "Iy9DmgIG27mw"
   ],
   "name": "Springboard Decision Tree Specialty Coffee Case Study - Tier 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
